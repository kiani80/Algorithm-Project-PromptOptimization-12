# src/llm_oracle.py
import random

# یک دیکشنری شامل ۱۰ پرامپت مختلف برای خلاصه‌سازی و امتیاز (Fitness Score) آن‌ها از ۱ تا ۱۰
# این امتیازها نشان‌دهنده کیفیت خلاصه‌ای است که هر پرامپت فرضی تولید می‌کند.
PROMPT_SCORES = {
    "خلاصه متن": 2,  # امتیاز پایین به دلیل کلی بودن
    "متن زیر را خلاصه کن": 3,
    "لطفا این مقاله را به صورت خلاصه بنویس": 4,
    "متن زیر را در یک پاراگراف کوتاه خلاصه کن": 5,
    "متن زیر را در سه خط خلاصه کن": 6,
    "مهم‌ترین نکات متن زیر را استخراج کرده و خلاصه کن": 7,
    "به عنوان یک ویراستار، متن را در دو خط خلاصه کن": 8,
    "مهم‌ترین وقایع، تاریخ‌ها و نام افراد را در ۳ خط خلاصه کن": 8, # مناسب برای تست متوسط ما
    "به عنوان یک استاد دانشگاه، متن را به زبان ساده و با حفظ اصطلاحات خلاصه کن": 9, # مناسب برای تست سخت ما
    "متن زیر را به صورت حرفه‌ای، بدون زیاده‌گویی و در سه خط برای مدیرم خلاصه کن": 10 # بهترین پرامپت (بهینه مطلق)
}

# تبدیل کلیدهای دیکشنری به یک لیست برای انتخاب رندوم
PROMPT_POOL = list(PROMPT_SCORES.keys())

def generate_mutations(current_prompt, n_mutations=2):
    """
    این تابع نقش LLM را برای تولید پرامپت‌های جدید (همسایه‌ها) بازی می‌کند.
    n_mutations: تعداد همسایه‌هایی که می‌خواهیم تولید کنیم (پیش‌فرض ۲).
    """
    # انتخاب تصادفی n_mutations پرامپت از بین استخر پرامپت‌ها
    # در دنیای واقعی، اینجا یک درخواست به API مدل زبانی ارسال می‌شود.
    mutations = random.sample(PROMPT_POOL, n_mutations)
    return mutations

def evaluate_prompt(prompt, dataset=None):
    """
    این تابع نقش ارزیاب (Evaluator) را بازی می‌کند.
    پرامپت را می‌گیرد و امتیاز آن را از دیکشنری برمی‌گرداند.
    """
    # در دنیای واقعی، پرامپت روی dataset اجرا شده و خروجی توسط LLM نمره می‌گیرد.
    # در اینجا، اگر پرامپت در دیکشنری ما بود، امتیازش را برمی‌گردانیم.
    # اگر نبود (مثلاً تولید یک کاندیدای نامفهوم)، نمره پایین ۱ می‌دهیم تا الگوریتم آن را رد کند.
    return PROMPT_SCORES.get(prompt, 1)

# --- یک تست کوچک برای اجرای مستقیم فایل ---
if __name__ == "__main__":
    test_prompt = "متن زیر را خلاصه کن"
    print(f"Current Prompt: {test_prompt}")
    print(f"Score: {evaluate_prompt(test_prompt)}\n")
    
    new_candidates = generate_mutations(test_prompt, 2)
    print("Generated Candidates:")
    for idx, candidate in enumerate(new_candidates):
        score = evaluate_prompt(candidate)
        print(f"  {idx+1}. '{candidate}' -> Score: {score}")